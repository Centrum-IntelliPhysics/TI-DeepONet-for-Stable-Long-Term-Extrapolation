{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import jit, vmap, pmap, grad, value_and_grad\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from typing import Callable, Tuple, List, Dict, Optional, Any, Sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the random seed and create a JAX key\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the KdV dataset\n",
    "data = jnp.load(\"data/data_kdv.npz\")\n",
    "u = data['u']  #Initial condition\n",
    "xt = data['xt']  #grid\n",
    "g_u = data['g_u']  #Output\n",
    "ns = 1000\n",
    "nx = 100\n",
    "nt = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include u in the output and remake the grid\n",
    "\n",
    "g_u = g_u.reshape(ns, nt, nx)   \n",
    "u_ = u[:, jnp.newaxis, :]\n",
    "\n",
    "g_u_new = jnp.concatenate([u_, g_u], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs = u, outputs = g_u_new\n",
    "inputs = u\n",
    "outputs = g_u_new\n",
    "\n",
    "#Only consider half of the data upto timestep = 100\n",
    "outputs = outputs[:,:100,]\n",
    "\n",
    "ns, nt, nx = outputs.shape\n",
    "print(f\"ns = {ns}, nt = {nt}, nx = {nx}\")\n",
    "\n",
    "#Create the grid with coordinate pairs (t,x) to feed into trunk network\n",
    "tspan = jnp.linspace(0, 5, g_u_new.shape[2])\n",
    "xspan = jnp.linspace(0, 1, nx)\n",
    "\n",
    "#Take only half of the temporal domain\n",
    "tspan = tspan[:100]\n",
    "\n",
    "#Create meshgrid for trunk net\n",
    "[t,x] = jnp.meshgrid(tspan, xspan, indexing = 'ij')\n",
    "grid = jnp.transpose(jnp.array([t.flatten(), x.flatten()]))\n",
    "print(grid.shape)\n",
    "print(grid)\n",
    "\n",
    "# Split the data into training (2000) and testing (500) samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test, train_indices, test_indices = \\\n",
    "            train_test_split(inputs, outputs, jnp.arange(ns), test_size=0.2, random_state=seed)\n",
    "\n",
    "#Reshape outputs from (ns, nt, nx) to (ns, nt*nx)\n",
    "outputs_train = outputs_train.reshape(outputs_train.shape[0], nt*nx)\n",
    "outputs_test = outputs_test.reshape(outputs_test.shape[0], nt*nx)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Inputs - train\n",
    "branch_inputs_train = inputs_train\n",
    "trunk_inputs_train = grid\n",
    "\n",
    "#Inspecting the shapes\n",
    "print(\"Shape of train branch inputs: \",branch_inputs_train.shape)\n",
    "print(\"Shape of train trunk inputs: \",trunk_inputs_train.shape)\n",
    "print(\"Shape of train output: \",outputs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Inputs - test\n",
    "branch_inputs_test = inputs_test      \n",
    "trunk_inputs_test = grid             \n",
    "\n",
    "#Inspecting the shapes\n",
    "print(\"Shape of test branch inputs: \",branch_inputs_test.shape)\n",
    "print(\"Shape of test trunk inputs: \",trunk_inputs_test.shape)\n",
    "print(\"Shape of test output: \",outputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility class for defining the branch network\n",
    "class branch_net(nn.Module):\n",
    "\n",
    "    layer_sizes: Sequence[int] \n",
    "    activation: Callable\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self,x):\n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        for layer in self.layer_sizes[:-1]:\n",
    "            x = nn.Dense(layer, kernel_init=init)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(self.layer_sizes[-1], kernel_init=init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility class for defining the trunk network\n",
    "class trunk_net(nn.Module):\n",
    "\n",
    "    layer_sizes: Sequence[int]    \n",
    "    activation: Callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,x):\n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        for layer in self.layer_sizes:\n",
    "            x = nn.Dense(layer, kernel_init=init)(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the DeepONet model\n",
    "class DeepONet(nn.Module):\n",
    "\n",
    "    branch_net_config: Sequence[int]\n",
    "    trunk_net_config: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self.branch_net = branch_net(self.branch_net_config, nn.swish)\n",
    "        self.trunk_net = trunk_net(self.trunk_net_config, nn.swish)\n",
    "\n",
    "\n",
    "    def __call__(self, x_branch, x_trunk):\n",
    "        \n",
    "        #Vectorize over multiple samples of input functions\n",
    "        branch_outputs = vmap(self.branch_net, in_axes = 0)(x_branch)\n",
    "        \n",
    "        #Vectorize over multiple query points\n",
    "        trunk_outputs = vmap(self.trunk_net, in_axes = 0)(x_trunk)\n",
    "        \n",
    "        inner_product = jnp.einsum('ik,jk->ij', branch_outputs, trunk_outputs)\n",
    "\n",
    "        return inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the latent dimension at the output of branch/trunk net\n",
    "latent_vector_size = 300\n",
    "\n",
    "#Create the branch and trunk layer configurations\n",
    "branch_network_layer_sizes = [150, 250, 450, 380, 320] + [latent_vector_size]\n",
    "trunk_network_layer_sizes = [200, 220, 240, 250, 260, 280] + [latent_vector_size]\n",
    "\n",
    "#Instantiate the DeepONet model\n",
    "model = DeepONet(branch_net_config = branch_network_layer_sizes, \n",
    "                                      trunk_net_config = trunk_network_layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to save model params\n",
    "def save_model_params(params, path, filename):\n",
    "    \n",
    "    #Create output directory for saving model params\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    save_path = os.path.join(path, filename)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "#Utility function to load model params\n",
    "def load_model_params(path, filename):\n",
    "    load_path = os.path.join(path, filename)\n",
    "    with open(load_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process from here\n",
    "@jax.jit\n",
    "def loss_fn(params, branch_inputs, trunk_inputs, gt_outputs):\n",
    "    predictions = model.apply(params, branch_inputs,trunk_inputs)\n",
    "    mse_loss = jnp.mean(jnp.square(predictions - gt_outputs))\n",
    "    return mse_loss\n",
    "\n",
    "@jax.jit\n",
    "def update(params, branch_inputs, trunk_inputs, gt_outputs, opt_state):\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, branch_inputs, trunk_inputs, gt_outputs)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Initialize model parameters\n",
    "params = model.init(key, branch_inputs_train, trunk_inputs_train)\n",
    "\n",
    "# # Optimizer setup\n",
    "lr_scheduler = optax.schedules.exponential_decay(init_value=1e-3, transition_steps=1000, decay_rate=0.95)\n",
    "optimizer = optax.adam(learning_rate=lr_scheduler)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "training_loss_history = []\n",
    "test_loss_history = []\n",
    "num_epochs = int(1e5)\n",
    "batch_size = 256\n",
    "\n",
    "min_test_loss = jnp.inf\n",
    "\n",
    "filepath = 'DeepONet_full_rollout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "\n",
    "    #Perform mini-batching\n",
    "    shuffled_indices = jax.random.permutation(random.PRNGKey(epoch), branch_inputs_train.shape[0])\n",
    "    batch_indices = shuffled_indices[:batch_size]\n",
    "\n",
    "    branch_inputs_train_batch = branch_inputs_train[batch_indices]\n",
    "    outputs_train_batch = outputs_train[batch_indices]\n",
    "\n",
    "    # Update the parameters and optimizer state\n",
    "    params, opt_state, loss = update(\n",
    "        params=params,\n",
    "        branch_inputs=branch_inputs_train_batch,\n",
    "        trunk_inputs=trunk_inputs_train,\n",
    "        gt_outputs=outputs_train_batch,\n",
    "        opt_state=opt_state\n",
    "    )\n",
    "\n",
    "    #Keep track of the train loss\n",
    "    training_loss_history.append(loss)\n",
    "    \n",
    "    #Do predictions on the test data simultaneously\n",
    "    test_mse_loss = loss_fn(params = params, \n",
    "                            branch_inputs = branch_inputs_test, \n",
    "                            trunk_inputs = trunk_inputs_test, \n",
    "                            gt_outputs = outputs_test)\n",
    "    test_loss_history.append(test_mse_loss)\n",
    "    \n",
    "    #Save the params of the best model encountered till now\n",
    "    if test_mse_loss < min_test_loss:\n",
    "        best_params = params\n",
    "        save_model_params(best_params, path = filepath, filename = 'model_params_best.pkl')\n",
    "        min_test_loss = test_mse_loss\n",
    "    \n",
    "    #Print the train and test loss history every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch}, training_loss_MSE: {loss}, test_loss_MSE: {test_mse_loss}, \\\n",
    "                  best_test_loss_MSE: {min_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the train and test loss histories\n",
    "plt.figure(dpi = 130)\n",
    "plt.semilogy(np.arange(num_epochs), training_loss_history, label = \"Train loss\")\n",
    "plt.semilogy(np.arange(num_epochs), test_loss_history, label = \"Test loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tick_params(which = 'major', axis = 'both', direction = 'in', length = 6)\n",
    "plt.tick_params(which = 'minor', axis = 'both', direction = 'in', length = 3.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.legend(loc = 'best')\n",
    "plt.savefig(filepath + \"/loss_plot.jpeg\", dpi = 800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the loss arrays\n",
    "np.save(filepath + \"/Train_loss.npy\", training_loss_history)\n",
    "np.save(filepath + \"/Test_loss.npy\", test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "filepath = 'DeepONet_full_rollout'\n",
    "#Import the best model saved after full training\n",
    "best_params = load_model_params(path = filepath, filename = 'model_params_best.pkl')\n",
    "\n",
    "#Ground truth output\n",
    "outputs = g_u_new\n",
    "\n",
    "branch_input_new = outputs[:, 0, :]\n",
    "tspan = jnp.linspace(0, 1, 201)\n",
    "xspan = jnp.linspace(0, 1, 100)\n",
    "\n",
    "#Create meshgrid (t, x) for trunk net\n",
    "[t_new, x_new] = jnp.meshgrid(tspan, xspan, indexing = 'ij')\n",
    "grid = jnp.transpose(jnp.array([t_new.flatten(), x_new.flatten()]))\n",
    "\n",
    "#Perform inferencing\n",
    "predictions_outputs_new = model.apply(best_params, branch_input_new, grid)\n",
    "predictions_outputs_new = predictions_outputs_new.reshape(predictions_outputs_new.shape[0], 201, 100)\n",
    "\n",
    "#Randomly selecting \"size\" number of samples out of the test dataset\n",
    "random_samples = np.random.choice(np.arange(outputs.shape[0]), size = 3, replace = 'True')\n",
    "\n",
    "for i in random_samples:\n",
    "    \n",
    "    prediction_i = predictions_outputs_new[i, :, :]\n",
    "    target_i = outputs[i, :, :]\n",
    "    \n",
    "    error_i = np.abs(prediction_i - target_i)\n",
    "    \n",
    "    plt.figure(figsize = (12,3))\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    contour1 = plt.contourf(xspan, tspan, prediction_i, levels = 20, cmap = 'jet')\n",
    "    cbar1 = plt.colorbar()\n",
    "    cbar1.ax.tick_params(labelsize = 12)\n",
    "    plt.xlabel(\"x\", fontsize = 14)\n",
    "    plt.ylabel(\"t\", fontsize = 14)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.title(\"Predicted\", fontsize = 16)\n",
    "    \n",
    "    plt.subplot(1,3,2)\n",
    "    contour2 = plt.contourf(xspan, tspan, target_i, levels = 20, cmap = 'jet')\n",
    "    cbar2 = plt.colorbar()\n",
    "    cbar2.ax.tick_params(labelsize = 12)\n",
    "    plt.xlabel(\"x\", fontsize = 14)\n",
    "    plt.ylabel(\"t\", fontsize = 14)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.title(\"Actual\", fontsize = 16)\n",
    "  \n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    contour3 = plt.contourf(xspan, tspan, error_i, levels = 20, cmap = 'Wistia')\n",
    "    cbar3 = plt.colorbar()\n",
    "    cbar3.ax.tick_params(labelsize = 12)\n",
    "    plt.xlabel(\"x\", fontsize = 14)\n",
    "    plt.ylabel(\"t\", fontsize = 14)\n",
    "    plt.xticks(fontsize = 12)\n",
    "    plt.yticks(fontsize = 12)\n",
    "    plt.title(\"Error\", fontsize = 16)\n",
    "    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath + f\"/Contour_plots_sidx_{i}.jpeg\", dpi = 800)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the relative L2 error incurred at every timestep\n",
    "\n",
    "auto_reg_error = []\n",
    "num_time_steps = 201\n",
    "\n",
    "for i in range(num_time_steps):\n",
    "    l2_error = jnp.linalg.norm(predictions_outputs_new[:,i,:] - outputs[:,i,:])/jnp.linalg.norm(outputs[:,i,:])\n",
    "    auto_reg_error.append(l2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the auto_reg_error array for comparing with TI-DON approach\n",
    "np.save(filepath + \"/Auto_reg_error_full_rollout.npy\", auto_reg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the predictions and ground truth outputs\n",
    "\n",
    "np.save(filepath + \"/u_pred.npy\", predictions_outputs_new)\n",
    "np.save(filepath + \"/u_actual.npy\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
