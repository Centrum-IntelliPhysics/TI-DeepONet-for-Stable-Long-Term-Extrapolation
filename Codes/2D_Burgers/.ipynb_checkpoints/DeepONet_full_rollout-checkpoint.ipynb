{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "\n",
    "import torch\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax import jit, vmap, pmap, grad, value_and_grad\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "from typing import Callable, Tuple, List, Dict, Optional, Any, Sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the random seed and create a JAX key\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "key = random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the 2D Burgers' dataset\n",
    "dataset = torch.load(\"Burgers_equation_2D_scalar.pt\")\n",
    "\n",
    "#Input initial conditions\n",
    "inputs = dataset['input_samples']\n",
    "\n",
    "#Ground truth output solution fields - #Ns = 5000, Nx = 32, Ny = 32, Nt = 101\n",
    "outputs = dataset['output_samples']\n",
    "\n",
    "#Defining t-domain, x-domain, y-domain\n",
    "tspan = jnp.linspace(0, 1, 101)\n",
    "xspan = jnp.linspace(0, 1, 32)\n",
    "yspan = jnp.linspace(0, 1, 32)\n",
    "\n",
    "#Convert to JAX NumPy arrays\n",
    "inputs = jnp.array(inputs)\n",
    "outputs = jnp.array(outputs)\n",
    "\n",
    "#Consider only 1000 samples for analysis (due to memory constraints)\n",
    "inputs = inputs[:1000, :, :]\n",
    "outputs = outputs[:1000, :, :]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Free memory by deleting the dataset\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only consider one-third of the data upto timestep = 33\n",
    "outputs = outputs[:,:33,:]\n",
    "\n",
    "ns, nt, nx, ny = outputs.shape\n",
    "print(f\"ns: {ns}, nt: {nt}, nx: {nx}, ny: {ny}\")\n",
    "\n",
    "#Take only one-third of the temporal domain\n",
    "tspan = tspan[:33]\n",
    "\n",
    "#Create for trunk network\n",
    "[t,x,y] = jnp.meshgrid(tspan, xspan, yspan, indexing = 'ij')\n",
    "grid = jnp.transpose(jnp.array([t.flatten(), x.flatten(), y.flatten()]))\n",
    "\n",
    "\n",
    "# Split the data into training (2000) and testing (500) samples\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = \\\n",
    "                    train_test_split(inputs, outputs, test_size=0.2, random_state=seed)\n",
    "\n",
    "#Reshape the ground truth output solution field from (ns, nt, nx, ny) to (ns, nt*nx*ny)\n",
    "outputs_train = outputs_train.reshape(outputs_train.shape[0], nt*nx*ny)\n",
    "outputs_test = outputs_test.reshape(outputs_test.shape[0], nt*nx*ny)\n",
    "\n",
    "# Check the shapes of the subsets\n",
    "print(\"Shape of inputs_train:\", inputs_train.shape)\n",
    "print(\"Shape of inputs_test:\", inputs_test.shape)\n",
    "print(\"Shape of outputs_train:\", outputs_train.shape)\n",
    "print(\"Shape of outputs_test:\", outputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Inputs - train\n",
    "branch_inputs_train = inputs_train    \n",
    "trunk_inputs_train = grid             \n",
    "\n",
    "#Inspecting the shapes\n",
    "print(\"Shape of train branch inputs: \",branch_inputs_train.shape)\n",
    "print(\"Shape of train trunk inputs: \",trunk_inputs_train.shape)\n",
    "print(\"Shape of train output: \",outputs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network Inputs - test\n",
    "branch_inputs_test = inputs_test      \n",
    "trunk_inputs_test = grid              \n",
    "\n",
    "#Inspecting the shapes\n",
    "print(\"Shape of test branch inputs: \",branch_inputs_test.shape)\n",
    "print(\"Shape of test trunk inputs: \",trunk_inputs_test.shape)\n",
    "print(\"Shape of test output: \",outputs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility class for defining the branch network\n",
    "class branch_net(nn.Module):\n",
    "\n",
    "    layer_sizes: Sequence[int] \n",
    "    activation: Callable\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        # #x has shape (ns, nx, ny) - so add channel dimension: (ns, nx, ny, nc)\n",
    "        x = x[..., jnp.newaxis]\n",
    "        \n",
    "        #2D Convolutional and pooling layers\n",
    "        x = nn.Conv(features = 64, kernel_size = (3,3), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides = (2, 2), padding = \"SAME\")\n",
    "        \n",
    "        x = nn.Conv(features = 64, kernel_size = (2, 2), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape = (2,2), strides = (2,2), padding = \"SAME\")\n",
    "         \n",
    "        x = x.flatten()   #flattening layer\n",
    "        \n",
    "        #MLP layers\n",
    "        for i, layer in enumerate(self.layer_sizes[:-1]):\n",
    "            x = nn.Dense(layer, kernel_init = init)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(self.layer_sizes[-1], kernel_init = init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility class for defining the trunk network\n",
    "class trunk_net(nn.Module):\n",
    "\n",
    "    layer_sizes: Sequence[int]    \n",
    "    activation: Callable\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        for i, layer in enumerate(self.layer_sizes):\n",
    "            x = nn.Dense(layer, kernel_init = init)(x)\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the DeepONet model\n",
    "class DeepONet(nn.Module):\n",
    "\n",
    "    branch_net_config: Sequence[int]\n",
    "    trunk_net_config: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self.branch_net = branch_net(self.branch_net_config, nn.activation.silu)\n",
    "        self.trunk_net = trunk_net(self.trunk_net_config, nn.activation.silu)\n",
    "\n",
    "    def __call__(self, x_branch, x_trunk):\n",
    "\n",
    "        #Vectorize over multiple samples of input functions\n",
    "        branch_outputs = vmap(self.branch_net, in_axes = 0)(x_branch)\n",
    "\n",
    "        #Vectorize over multiple query points\n",
    "        trunk_outputs = vmap(self.trunk_net, in_axes = 0)(x_trunk)\n",
    "        \n",
    "        inner_product = jnp.einsum('ik,jk->ij', branch_outputs, trunk_outputs)\n",
    "\n",
    "        return inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the latent dimension at the output of branch/trunk net\n",
    "latent_vector_size = 100\n",
    "\n",
    "#Create the branch and trunk layer configurations\n",
    "#Note that the config for the 2D Conv and pooling layers is hard coded into the branch net class\n",
    "branch_network_layer_sizes = [256, 128] + [latent_vector_size]\n",
    "trunk_network_layer_sizes = [128]*3 + [latent_vector_size]\n",
    "\n",
    "#Instantiate the DeepONet model\n",
    "model = DeepONet(branch_net_config = branch_network_layer_sizes, \n",
    "                                      trunk_net_config = trunk_network_layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to save model params\n",
    "def save_model_params(params, path, filename):\n",
    "    \n",
    "    #Create output directory for saving model params\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    save_path = os.path.join(path, filename)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "#Utility function to load model params\n",
    "def load_model_params(path, filename):\n",
    "    load_path = os.path.join(path, filename)\n",
    "    with open(load_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training process from here\n",
    "@jax.jit\n",
    "def loss_fn(params, branch_inputs, trunk_inputs, gt_outputs):\n",
    "    predictions = model.apply(params, branch_inputs,trunk_inputs)\n",
    "    mse_loss = jnp.mean(jnp.square(predictions - gt_outputs))   \n",
    "    l2_error = jnp.linalg.norm(predictions - gt_outputs)/jnp.linalg.norm(gt_outputs)\n",
    "    return mse_loss, l2_error\n",
    "\n",
    "@jax.jit\n",
    "def update(params, branch_inputs, trunk_inputs, gt_outputs, opt_state):\n",
    "    (loss, l2_error), grads = \\\n",
    "            jax.value_and_grad(loss_fn, has_aux=True)(params, branch_inputs, trunk_inputs, gt_outputs)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss, l2_error\n",
    "\n",
    "# Initialize model parameters\n",
    "params = model.init(key, branch_inputs_train, trunk_inputs_train)\n",
    "\n",
    "# # Optimizer setup\n",
    "lr_scheduler = optax.schedules.exponential_decay(init_value = 1e-3, transition_steps = 5000, decay_rate = 0.95)\n",
    "optimizer = optax.adam(learning_rate=lr_scheduler)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "training_loss_history = []\n",
    "test_loss_history = []\n",
    "num_epochs = int(1e5)\n",
    "batch_size = 128\n",
    "\n",
    "min_test_mse_loss = jnp.inf\n",
    "\n",
    "filepath = 'DeepONet_full_rollout'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeing memory by deleting inputs and outputs\n",
    "\n",
    "del inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "\n",
    "    #Perform mini-batching\n",
    "    shuffled_indices = jax.random.permutation(jax.random.PRNGKey(epoch), branch_inputs_train.shape[0])\n",
    "    batch_indices = shuffled_indices[:batch_size]\n",
    "\n",
    "    branch_inputs_train_batch = branch_inputs_train[batch_indices]\n",
    "    outputs_train_batch = outputs_train[batch_indices]\n",
    "\n",
    "    # Update the parameters and optimizer state\n",
    "    params, opt_state, loss, l2_error = update(\n",
    "        params=params,\n",
    "        branch_inputs=branch_inputs_train_batch,\n",
    "        trunk_inputs=trunk_inputs_train,\n",
    "        gt_outputs=outputs_train_batch,\n",
    "        opt_state=opt_state\n",
    "    )\n",
    "\n",
    "    #Keep a track of the training loss\n",
    "    training_loss_history.append(loss)\n",
    "    \n",
    "    #Do predictions on the test data simultaneously\n",
    "    test_mse_loss, test_l2_error = loss_fn(params = params, \n",
    "                            branch_inputs = branch_inputs_test, \n",
    "                            trunk_inputs = trunk_inputs_test, \n",
    "                            gt_outputs = outputs_test)\n",
    "    test_loss_history.append(test_mse_loss)\n",
    "    \n",
    "    #Save the params of the best model encountered till now    \n",
    "    if test_mse_loss < min_test_mse_loss:\n",
    "        best_params = params\n",
    "        save_model_params(best_params, path = filepath, filename = 'model_params_best.pkl')\n",
    "        min_test_mse_loss = test_mse_loss\n",
    "        \n",
    "    \n",
    "    #Print the train and test loss history every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch}, training_loss_MSE: {loss}, test_loss: {test_mse_loss}, \\\n",
    "              min_test_loss: {min_test_mse_loss}, min_test_l2_error: {min_test_l2_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the train and test loss histories\n",
    "plt.figure(dpi = 130)\n",
    "plt.semilogy(np.arange(epoch+1), training_loss_history, label = \"Train loss\")\n",
    "plt.semilogy(np.arange(epoch+1), test_loss_history, label = \"Test loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tick_params(which = 'major', axis = 'both', direction = 'in', length = 6)\n",
    "plt.tick_params(which = 'minor', axis = 'both', direction = 'in', length = 3.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.legend(loc = 'best')\n",
    "plt.savefig(filepath + \"/loss_plot.jpeg\", dpi = 800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the loss arrays\n",
    "np.save(filepath + \"/train_loss.npy\", training_loss_history)\n",
    "np.save(filepath + \"/test_loss.npy\", test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the required datasets afresh for performing inferencing\n",
    "dataset = torch.load(\"Burgers_equation_2D_scalar.pt\")\n",
    "inputs = dataset['input_samples']\n",
    "outputs = dataset['output_samples']\n",
    "\n",
    "inputs = jnp.array(inputs)\n",
    "outputs = jnp.array(outputs)\n",
    "\n",
    "inputs = inputs[:1000]\n",
    "outputs = outputs[:1000]\n",
    "\n",
    "#Free memory by deleting dataset\n",
    "del dataset\n",
    "\n",
    "tspan = jnp.linspace(0, 1, 101)\n",
    "xspan = jnp.linspace(0, 1, 32)\n",
    "yspan = jnp.linspace(0, 1, 32)\n",
    "\n",
    "#Create for trunk network\n",
    "[t,x,y] = jnp.meshgrid(tspan, xspan, yspan, indexing = 'ij')\n",
    "grid = jnp.transpose(jnp.array([t.flatten(), x.flatten(), y.flatten()]))\n",
    "\n",
    "#Creating grid for branch inputs new and trunk_inputs_new\n",
    "branch_inputs_new = inputs\n",
    "trunk_inputs_new = grid\n",
    "trunk_inputs_new.shape, trunk_inputs_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "filepath = 'DeepONet_full_rollout'\n",
    "\n",
    "#Import the best model saved after full training\n",
    "best_params = load_model_params(path = filepath, filename = 'model_params_best.pkl')\n",
    "\n",
    "\n",
    "#Perform inferencing\n",
    "predictions_outputs_new = model.apply(best_params, branch_inputs_new, trunk_inputs_new)\n",
    "predictions_outputs_new = predictions_outputs_new.reshape(predictions_outputs_new.shape[0], 101, 32, 32)\n",
    "\n",
    "#Randomly selecting \"size\" number of samples out of the test dataset\n",
    "random_samples = np.random.choice(np.arange(outputs.shape[0]), size = 3, replace = 'True')\n",
    "\n",
    "t_query = [0, 20, 25, 50, -1]\n",
    "\n",
    "for i in random_samples:\n",
    "    \n",
    "    for t in t_query:\n",
    "        prediction_i = predictions_outputs_new[i, t, :, :]\n",
    "        target_i = outputs[i, t, :, :]\n",
    "        \n",
    "\n",
    "        error_i = np.abs(prediction_i - target_i)\n",
    "\n",
    "        plt.figure(figsize = (12,3))\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        contour1 = plt.contourf(xspan, yspan, prediction_i, levels = 20, cmap = 'jet')\n",
    "        cbar1 = plt.colorbar()\n",
    "        cbar1.ax.tick_params(labelsize = 12)\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        plt.title(\"Predicted\", fontsize = 16)\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        contour2 = plt.contourf(xspan, yspan, target_i, levels = 20, cmap = 'jet')\n",
    "        cbar2 = plt.colorbar()\n",
    "        cbar2.ax.tick_params(labelsize = 12)\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        plt.title(\"Actual\", fontsize = 16)\n",
    "\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        contour3 = plt.contourf(xspan, yspan, error_i, levels = 20, cmap = 'Wistia')\n",
    "        cbar3 = plt.colorbar()\n",
    "        cbar3.ax.tick_params(labelsize = 12)\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        plt.title(\"Error\", fontsize = 16)\n",
    "        \n",
    "        plt.suptitle(f\"Idx: {i}, timestep: {t}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath + f\"/Contour_plots_sidx_{i}_timestep_{t}.jpeg\", dpi = 800)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the relative L2 error incurred at every timestep\n",
    "\n",
    "auto_reg_error = []\n",
    "num_time_steps = 101\n",
    "\n",
    "for i in range(num_time_steps):\n",
    "    l2_error = jnp.linalg.norm(predictions_outputs_new[:,i,:,:] - outputs[:,i,:,:])/jnp.linalg.norm(outputs[:,i,:,:])\n",
    "    auto_reg_error.append(l2_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the auto_reg_error array for comparing with NODE approach\n",
    "np.save(filepath + \"/Auto_reg_error_full_rollout.npy\", auto_reg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the u_pred and ground truth output arrays for separate postprocessing\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    np.save(filepath + \"/u_pred.npy\", u_pred)\n",
    "    np.save(filepath + \"/u_actual.npy\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
