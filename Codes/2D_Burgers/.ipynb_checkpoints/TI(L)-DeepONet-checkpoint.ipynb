{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c3477-9a71-4a31-89d1-6b52d1ca8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "import os, sys, pickle\n",
    "import jax, jaxlib\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd08a40d-eb42-4da3-b805-23f6c310c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a random seed and create the JAX PRNG Key\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "key = jax.random.PRNGKey(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab6f98-dfdc-41b5-a8d6-43bb8f47e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the 2D Burgers' dataset\n",
    "\n",
    "dataset = torch.load(\"Burgers_equation_2D_scalar.pt\")\n",
    "inputs = dataset['input_samples']\n",
    "outputs = dataset['output_samples']    \n",
    "\n",
    "inputs = jnp.array(inputs)\n",
    "outputs = jnp.array(outputs)    #Ns = 5000, Nt = 101, Nx = 32, Ny = 32\n",
    "\n",
    "#Consider first 1000 samples due to memory constraints\n",
    "inputs = inputs[:1000, :, :]\n",
    "outputs = outputs[:1000, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf784a-ffc6-4119-abcf-c063d061c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the dataset and free up memory\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043350c-c2c9-4ebc-bf28-3070a3a34a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns, nt, nx, ny = outputs.shape\n",
    "print(f\"ns: {ns}, nt: {nt}, nx: {nx}, ny: {ny}\")\n",
    "\n",
    "'''\n",
    "Now, we need to create a training data where input is [(u0, u1, u2, u3, ...., u33)] and\n",
    "output is [(u1, u2, u3,....., u34)]\n",
    "'''\n",
    "\n",
    "#Creating the input and output training data\n",
    "init_timestep = 0\n",
    "end_timestep = 33\n",
    "\n",
    "input_data_NN = outputs[:, init_timestep, :, :]    \n",
    "output_data_NN = outputs[:, init_timestep+1, :, :]\n",
    "\n",
    "for i in range(init_timestep+1, end_timestep):\n",
    "    input_data_NN = jnp.vstack((input_data_NN, outputs[:,i,:,:]))\n",
    "    output_data_NN = jnp.vstack((output_data_NN, outputs[:,i+1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195006c-f681-4065-abc4-673a03d9e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the output_data_NN from (ns*nt//3, nx, ny) to (ns*nt//3, nx*ny)\n",
    "#Input_data_NN remains as it is, i.e., (ns*nt//3, nx, ny)\n",
    "output_data_NN = output_data_NN.reshape(output_data_NN.shape[0], output_data_NN.shape[1]*output_data_NN.shape[2])\n",
    "input_data_NN.shape, output_data_NN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8e0be-26bb-4e55-96fc-49d285afc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training and testing data splits\n",
    "input_data_NN_train, input_data_NN_test, output_data_NN_train, output_data_NN_test = \\\n",
    "                        train_test_split(input_data_NN, output_data_NN, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e16ac1-98ac-4f15-91ed-5d5509d7e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeing memory by deleting input_data_NN and output_data_NN\n",
    "del input_data_NN, output_data_NN, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e45dec-523f-4643-80ab-a39664895065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for defining the branch network\n",
    "class branch_net(nn.Module):\n",
    "\n",
    "    layer_sizes: Sequence[int] \n",
    "    activation: Callable\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        # #x has shape (ns, nx, ny) - so add channel dimension: (ns, nx, ny, nc)\n",
    "        x = x[..., jnp.newaxis]\n",
    "        \n",
    "        #2D Convolutional layers and pooling layers\n",
    "        x = nn.Conv(features = 64, kernel_size = (3,3), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides = (2, 2), padding = \"SAME\")\n",
    "        \n",
    "        x = nn.Conv(features = 64, kernel_size = (2, 2), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape = (2,2), strides = (2,2), padding = \"SAME\")\n",
    "        \n",
    "        x = x.flatten()   #flattening layer\n",
    "        \n",
    "        #MLP layers\n",
    "        for i, layer in enumerate(self.layer_sizes[:-1]):\n",
    "            x = nn.Dense(layer, kernel_init = init)(x)\n",
    "            x = self.activation(x)\n",
    "        x = nn.Dense(self.layer_sizes[-1], kernel_init = init)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96303dce-7c57-4105-b3d2-02bf329fee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for defining the trunk network\n",
    "class trunk_net(nn.Module):\n",
    "    trunk_layer_config: Sequence[int]\n",
    "    activation: Callable\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        #Trunk network forward pass\n",
    "        for i, layer_size in enumerate(self.trunk_layer_config):\n",
    "            x = nn.Dense(layer_size, kernel_init = init)(x)\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2288c69-dcdd-452c-977e-51046cf151bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the DeepONet model\n",
    "class DeepONet(nn.Module):\n",
    "\n",
    "    branch_net_config: Sequence[int]\n",
    "    trunk_net_config: Sequence[int]\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        self.branch_net = branch_net(self.branch_net_config, nn.activation.tanh)\n",
    "        self.trunk_net = trunk_net(self.trunk_net_config, nn.activation.tanh)\n",
    "\n",
    "\n",
    "    def __call__(self, x_branch, x_trunk):\n",
    "        \n",
    "        #Vectorize over multiple samples of input functions\n",
    "        branch_outputs = jax.vmap(self.branch_net, in_axes = 0)(x_branch)\n",
    "        \n",
    "        #Vectorize over multiple query points\n",
    "        trunk_outputs = jax.vmap(self.trunk_net, in_axes = 0)(x_trunk)       \n",
    "        \n",
    "        inner_product = jnp.einsum('ik,jk->ij', branch_outputs, trunk_outputs)\n",
    "\n",
    "        return inner_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24243a0-5d38-4c4c-8c0d-a4e61fe08981",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the auxiliary NN for predicting the adaptive RK4 slope coefficients\n",
    "class LearnableRK4(nn.Module):\n",
    "    hidden_dim: int = 32\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, u_curr):\n",
    "        \n",
    "        init = nn.initializers.glorot_normal()\n",
    "        \n",
    "        x = u_curr\n",
    "        \n",
    "        #u_curr is [bs, nx, ny]. So, add a channel dimension to make it [bs, nx, ny, 1]\n",
    "        x = x[..., jnp.newaxis]\n",
    "        \n",
    "        #2D Convolutional layers and pooling layers\n",
    "        x = nn.Conv(features = self.hidden_dim, kernel_size = (3, 3), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.max_pool(x, window_shape=(2, 2), strides = (2, 2), padding = \"SAME\")\n",
    "        \n",
    "        x = nn.Conv(features = self.hidden_dim, kernel_size = (2, 2), strides = 1, padding = \"SAME\")(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, window_shape=(2, 2), strides = (2, 2), padding = \"SAME\")\n",
    "        \n",
    "        x = x.flatten()  #Flattening layer\n",
    "\n",
    "        #MLP layers\n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.activation.tanh(x)\n",
    "        \n",
    "        x = nn.Dense(self.hidden_dim)(x)\n",
    "        x = nn.activation.tanh(x)\n",
    "        \n",
    "        x = nn.Dense(4)(x)\n",
    "        x = nn.activation.softmax(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4bc11a-d20b-49a4-b182-b5391a7c6566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform one step of adaptive RK4\n",
    "def dynamic_rk4_step(u_curr, model_fn, params, model_rk_fn, rk_params, trunk_inputs, dt):\n",
    "    \n",
    "    alpha = jax.vmap(model_rk_fn, in_axes = (None, 0))(rk_params, u_curr)         #(Shape: (batch_size, 4)\n",
    "    \n",
    "    #Extract the coefficients  - each with shape (batch_size, 1)\n",
    "    alpha1 = alpha[:,0:1,None]\n",
    "    alpha2 = alpha[:,1:2,None]\n",
    "    alpha3 = alpha[:,2:3,None]\n",
    "    alpha4 = alpha[:,3:,None]\n",
    "\n",
    "    #Get the RK4 slopes\n",
    "    k1 = model_fn(params, u_curr, trunk_inputs)\n",
    "    k1 = k1.reshape(k1.shape[0], nx, ny)\n",
    "    \n",
    "    k2 = model_fn(params, u_curr + 0.5 * dt * k1, trunk_inputs)\n",
    "    k2 = k2.reshape(k2.shape[0], nx, ny)\n",
    "    \n",
    "    k3 = model_fn(params, u_curr + 0.5 * dt * k2, trunk_inputs)\n",
    "    k3 = k3.reshape(k3.shape[0], nx, ny)\n",
    "    \n",
    "    k4 = model_fn(params, u_curr + dt * k3, trunk_inputs)\n",
    "    k4 = k4.reshape(k4.shape[0], nx, ny)\n",
    "\n",
    "    u_next = u_curr + dt * (alpha1 * k1 + alpha2 * k2 + alpha3 * k3 + alpha4 * k4)\n",
    "    return u_next    #(ns*nt, nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1ed67-7782-4865-b7b9-cd2e30f62c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_fn(params, rk_params, branch_inputs, trunk_inputs, gt_outputs, dt=0.01):\n",
    "    \n",
    "    u_curr = branch_inputs  # Current state input (e.g., u(t))\n",
    "    u_next = gt_outputs     # Ground truth next state (e.g., u(t+1))\n",
    "    \n",
    "    u_pred_next = dynamic_rk4_step(u_curr, model_fn, params, model_rk_fn, rk_params, trunk_inputs, dt)\n",
    "    \n",
    "    #Reshape u_pred_next to match compatibility of u_next\n",
    "    u_pred_next = u_pred_next.reshape(u_pred_next.shape[0], nx*ny)\n",
    "\n",
    "    # Compute the Mean Squared Error loss between the predicted and ground truth next states\n",
    "    mse_loss = jnp.mean(jnp.square(u_pred_next - u_next))\n",
    "    \n",
    "    return mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e17841-8be1-40f8-a2ce-11d5751f7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update(params, rk_params, branch_inputs, trunk_inputs, gt_outputs, opt_state, opt_state_rk):\n",
    "    \n",
    "    #Update for DeepONet params\n",
    "    loss, grads = jax.value_and_grad(loss_fn, argnums = 0)(params, rk_params, branch_inputs, trunk_inputs, gt_outputs)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    #Update for RK params\n",
    "    _, rk_grads = jax.value_and_grad(loss_fn, argnums = 1)(params, rk_params, branch_inputs, trunk_inputs, gt_outputs)\n",
    "    updates_rk, opt_state_rk = optimizer_rk.update(rk_grads, opt_state_rk)\n",
    "    rk_params = optax.apply_updates(rk_params, updates_rk)\n",
    "    \n",
    "    return params, rk_params, opt_state, opt_state_rk, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8b1c3-add0-4a3f-8e16-52ec3b56d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form branch and trunk inputs train\n",
    "xspan = jnp.linspace(0, 1, nx)\n",
    "yspan = jnp.linspace(0, 1, ny)\n",
    "\n",
    "#Create for trunk network - takes a meshgrid of spatial coordinates only\n",
    "[x,y] = jnp.meshgrid(xspan, yspan, indexing = 'ij')\n",
    "grid = jnp.transpose(jnp.array([x.flatten(), y.flatten()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b62a5-5cc6-497f-8db1-ec5ad33a47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training data for branch and trunk inputs\n",
    "branch_inputs_train = input_data_NN_train\n",
    "trunk_inputs_train = grid\n",
    "outputs_train = output_data_NN_train\n",
    "\n",
    "print(\"Shape of branch inputs train: \",branch_inputs_train.shape)\n",
    "print(\"Shape of trunk inputs train: \",trunk_inputs_train.shape)\n",
    "print(\"Shape of outputs train: \",outputs_train.shape)\n",
    "print(\"Shape of grid: \",grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed59f1-8d2d-4489-9544-0c10ef87e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For branch and trunk inputs test\n",
    "\n",
    "branch_inputs_test = input_data_NN_test\n",
    "trunk_inputs_test = grid\n",
    "outputs_test = output_data_NN_test\n",
    "\n",
    "print(\"Shape of branch inputs test: \",branch_inputs_test.shape)\n",
    "print(\"Shape of trunk inputs test: \",trunk_inputs_test.shape)\n",
    "print(\"Shape of outputs test: \",outputs_test.shape)\n",
    "print(\"Shape of grid: \",grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa88bb9-8993-4e35-8bba-92ab72a6ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepONet settings\n",
    "\n",
    "#Define the latent dimension at the output of the branch/trunk net\n",
    "latent_vector_size = 100\n",
    "\n",
    "#Define the branch and trunnk network layer configurations\n",
    "branch_network_layer_sizes = [256, 128] + [latent_vector_size]\n",
    "trunk_network_layer_sizes = [128]*4 + [latent_vector_size]\n",
    "\n",
    "#Instantiate the DeepONet model\n",
    "model = DeepONet(branch_network_layer_sizes, trunk_network_layer_sizes)\n",
    "\n",
    "#Create a jitted function of the DeepONet model forward pass\n",
    "model_fn = jax.jit(model.apply)\n",
    "\n",
    "#Instantiate the learnable RK4 NN for the adaptive slope coefficients\n",
    "model_rk = LearnableRK4()\n",
    "\n",
    "#Create a jitted function fo the learnable RK4 NN model forward pass\n",
    "model_rk_fn = jax.jit(model_rk.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d041ae7c-e543-4a72-9697-613eae30cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for saving the model params\n",
    "def save_model_params(params, path, filename):\n",
    "    \n",
    "    #Create output directory for saving model params\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    save_path = os.path.join(path, filename)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "#Utility function for loading the model params\n",
    "def load_model_params(path, filename):\n",
    "    load_path = os.path.join(path, filename)\n",
    "    with open(load_path, 'rb') as f:\n",
    "        params = pickle.load(f)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc71764-30ac-4e16-9395-ab9b3783602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters - DeepONet and learnable RK4 NN\n",
    "key, subkey = jax.random.split(key)\n",
    "params = model.init(key, branch_inputs_train[0:1, ...], trunk_inputs_train[0:1, ...])\n",
    "\n",
    "#Make branch_inputs_train compatible for RK MLP\n",
    "rk_init = branch_inputs_train[0:1, ...]\n",
    "rk_params = model_rk.init(subkey, rk_init)\n",
    "\n",
    "# Optimizer setup\n",
    "\n",
    "#Initialize optimizer for DeepONet\n",
    "lr_scheduler = optax.schedules.exponential_decay(init_value=1e-3, transition_steps=5000, decay_rate=0.95)\n",
    "optimizer = optax.adam(learning_rate=lr_scheduler)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "#Initialize optimizer for RK4\n",
    "rk_lr_scheduler = optax.schedules.exponential_decay(init_value=5e-3, transition_steps=5000, decay_rate=0.95)\n",
    "optimizer_rk = optax.adam(learning_rate=rk_lr_scheduler)\n",
    "opt_state_rk = optimizer_rk.init(rk_params)\n",
    "\n",
    "training_loss_history = []\n",
    "test_loss_history = []\n",
    "num_epochs = int(1.5e5)\n",
    "batch_size = 64\n",
    "\n",
    "min_test_loss = jnp.inf\n",
    "\n",
    "filepath = 'TI-DON_learnableRK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f1176-64da-4a1a-8501-98105f783299",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n",
    "\n",
    "    #Perform mini-batching\n",
    "    shuffled_indices = jax.random.permutation(jax.random.PRNGKey(epoch), branch_inputs_train.shape[0])\n",
    "    batch_indices = shuffled_indices[:batch_size]\n",
    "\n",
    "    branch_inputs_train_batch = branch_inputs_train[batch_indices]\n",
    "    outputs_train_batch = outputs_train[batch_indices]\n",
    "\n",
    "    # Update the parameters and optimizer state\n",
    "    params, rk_params, opt_state, opt_state_rk, loss = update(\n",
    "        params=params,\n",
    "        rk_params=rk_params,\n",
    "        branch_inputs=branch_inputs_train_batch,\n",
    "        trunk_inputs=trunk_inputs_train,\n",
    "        gt_outputs=outputs_train_batch,\n",
    "        opt_state=opt_state,\n",
    "        opt_state_rk=opt_state_rk\n",
    "    )\n",
    "\n",
    "    #Keep a track of the train loss\n",
    "    training_loss_history.append(loss)\n",
    "    \n",
    "    #Do predictions on the test data simultaneously\n",
    "    test_mse_loss = loss_fn(params = params, \n",
    "                            rk_params = rk_params,\n",
    "                            branch_inputs = branch_inputs_test, \n",
    "                            trunk_inputs = trunk_inputs_test, \n",
    "                            gt_outputs = outputs_test)\n",
    "    test_loss_history.append(test_mse_loss)\n",
    "    \n",
    "    #Save the params of the best model encountered till now\n",
    "    if test_mse_loss < min_test_loss:\n",
    "        best_params = {\"deeponet_params\": params, \"rk_params\": rk_params}\n",
    "        save_model_params(best_params, path = filepath, filename = 'model_params_best.pkl')\n",
    "        min_test_loss = test_mse_loss\n",
    "    \n",
    "    #Print the train and test loss history every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch}, training_loss_MSE: {loss}, test_loss_MSE: {test_mse_loss}, \\\n",
    "                                best_test_loss_MSE: {min_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b79c1b-c568-4a56-8ffd-1a7ee22115e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the train and test loss histories\n",
    "plt.figure(dpi = 130)\n",
    "plt.semilogy(np.arange(epoch+1), training_loss_history, label = \"Train loss\")\n",
    "plt.semilogy(np.arange(epoch+1), test_loss_history, label = \"Test loss\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.tick_params(which = 'major', axis = 'both', direction = 'in', length = 6)\n",
    "plt.tick_params(which = 'minor', axis = 'both', direction = 'in', length = 3.5)\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.grid(alpha = 0.3)\n",
    "plt.legend(loc = 'best')\n",
    "plt.savefig(filepath + \"/loss_plot.jpeg\", dpi = 800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c8689-7fd4-4ee6-9c17-8438c371288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the loss arrays\n",
    "np.save(filepath + \"/Train_loss.npy\", training_loss_history)\n",
    "np.save(filepath + \"/Test_loss.npy\", test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ca5418-ca2b-4067-9941-bfdbb61a40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need to modify inferencing code as now we can use the learnt RK4 coefficients and do RK4 in prediction\n",
    "#Instead of AB-AM predictor-corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8fbe82-83de-4a81-aa8c-52bcc365783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform one inferencing step of adaptive RK4\n",
    "@jax.jit\n",
    "def inference(u_curr, trunk_inputs_test, dt=0.01):\n",
    "    u_next = dynamic_rk4_step(u_curr, model_fn, best_params, model_rk_fn, \n",
    "                              best_rk_params, trunk_inputs_test, dt)\n",
    "    return u_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431f55e7-1c35-4463-8899-adbb55796cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function for performing inferencing over all timesteps\n",
    "def run_inference(initial_u, trunk_inputs_test, n_steps, dt=0.01):\n",
    "    u_states = np.zeros(shape = (ns, nt, nx, ny))  # Array to store the states over time\n",
    "    u_states[:,0,:,:] = initial_u\n",
    "    \n",
    "    # Initialize the current state (this could be your u_0 and u_1, etc.)\n",
    "    u_curr = initial_u  # Set the current state to the initial state\n",
    "    \n",
    "    for i in range(1, n_steps):\n",
    "        # Perform one inference step using the adaptive RK4 method\n",
    "        u_next = inference(u_curr, trunk_inputs_test, dt)\n",
    "        \n",
    "        # Assign the predicted state\n",
    "        u_states[:, i, :, :] = u_next\n",
    "        \n",
    "        # Update current state for the next step\n",
    "        u_curr = u_next\n",
    "    \n",
    "    return u_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4847ca8d-4db3-406e-a25c-c47f91247568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model parameters\n",
    "best_full_params = load_model_params(path=filepath, filename='model_params_best.pkl')\n",
    "best_params = best_full_params['deeponet_params']\n",
    "best_rk_params = best_full_params['rk_params']\n",
    "\n",
    "#Reload all the relevant data for performing inferencing afresh\n",
    "dataset = torch.load(\"Burgers_equation_2D_scalar.pt\")\n",
    "inputs = dataset['input_samples']\n",
    "outputs = dataset['output_samples']\n",
    "\n",
    "inputs = jnp.array(inputs)\n",
    "outputs = jnp.array(outputs)\n",
    "\n",
    "#Consider first 1000 samples due to memory constraints\n",
    "inputs = inputs[:1000, :, :]\n",
    "outputs = outputs[:1000, :, :]\n",
    "\n",
    "del dataset\n",
    "\n",
    "#Start with u(t=0, x, y)\n",
    "u_curr = outputs[:, 0, :, :]\n",
    "\n",
    "#Perform inference\n",
    "u_pred = run_inference(u_curr, trunk_inputs_test, n_steps=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096e4ce-ce9a-43e4-afa0-94d3f4de973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly selecting \"size\" number of samples out of the test dataset\n",
    "indices = np.random.choice(np.arange(u_pred.shape[0]), size = 3, replace = 'False')\n",
    "\n",
    "x_test = jnp.linspace(0, 1, nx)\n",
    "y_test = jnp.linspace(0, 1, ny)\n",
    "t_test = jnp.linspace(0, 1, nt)\n",
    "\n",
    "t_query = [25, 50, -1]\n",
    "\n",
    "for idx in indices:\n",
    "    \n",
    "    for t in t_query:\n",
    "        plt.figure(figsize = (12,3))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        contour1 = plt.contourf(x_test, y_test, u_pred[idx, t, :, :], levels = 20, cmap = 'jet')\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        cbar1 = plt.colorbar()\n",
    "        cbar1.ax.tick_params(labelsize=12)\n",
    "        plt.title(\"Predicted\", fontsize = 16)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        contour2 = plt.contourf(x_test, y_test, outputs[idx, t, :, :], levels = 20, cmap = 'jet')\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        cbar2 = plt.colorbar()\n",
    "        cbar2.ax.tick_params(labelsize=12)\n",
    "        plt.title(\"Actual\", fontsize=16)\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        contour3 = plt.contourf(x_test, y_test, jnp.abs(u_pred[idx,t, :, :] - \n",
    "                                                        outputs[idx,t, :, :]), cmap = 'Wistia')\n",
    "        plt.xlabel(\"x\", fontsize = 14)\n",
    "        plt.ylabel(\"y\", fontsize = 14)\n",
    "        plt.xticks(fontsize = 12)\n",
    "        plt.yticks(fontsize = 12)\n",
    "        cbar3 = plt.colorbar()\n",
    "        cbar3.ax.tick_params(labelsize=12)\n",
    "        plt.title(\"Error\", fontsize = 16)\n",
    "        \n",
    "        plt.suptitle(f\"Sample Idx: {idx}, Timestep: {t}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig(filepath + f\"/Contour_plots_sidx_{idx}.jpeg\", dpi = 800)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0040c8c7-8822-4033-ad2e-47af9917eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the relative L2 error obtained at every timestep to show accummulation of autoregressive error\n",
    "\n",
    "auto_reg_error = []\n",
    "num_time_steps = 101\n",
    "\n",
    "for i in range(num_time_steps):\n",
    "    l2_error = jnp.linalg.norm(u_pred[:,i,:, :] - outputs[:,i,:,:])/jnp.linalg.norm(outputs[:,i,:,:])\n",
    "    auto_reg_error.append(l2_error)\n",
    "    \n",
    "plt.plot(jnp.arange(num_time_steps), auto_reg_error)\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Relative L2 error\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb77273-5aae-433d-9261-65bfc4cda887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the auto_reg_error array for comparing with TI approach\n",
    "np.save(filepath + \"/Auto_reg_error_with_TI-DON_learnableRK4.npy\", auto_reg_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d64675-a1e0-4c17-bae4-0a15d31dd62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the u_pred and ground truth output arrays for separate postprocessing\n",
    "\n",
    "save = True\n",
    "if save:\n",
    "    np.save(filepath + \"/u_pred.npy\", u_pred)\n",
    "    np.save(filepath + \"/actual.npy\", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
